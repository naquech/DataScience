{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommendation System in PySpark - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this last lab, we will implement a a movie recommendation system using ALS in Spark programming environment. Spark's machine learning libraray `Mllib` comes packaged with a very efficient imeplementation of ALS algorithm that we looked at in our previous lesson. The lab will require you to put into pratice your spark programming skills for creating and manipulating RDDs. We will go through a step-by-step process into developing a movie recommendation system using a larger subset of the MovieLens dataset than the one we used in previous lab.\n",
    "\n",
    "Note: You are advised to refer to [PySpark Documentation](http://spark.apache.org/docs/2.2.0/api/python/index.html) heavily for completing this lab as it will introduce a few new methods. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Demonsrate an understanding on how recommendation systems are being used for personalization of online services/products\n",
    "* Parse and filter datasets into Spark RDDs, performing basic feature selection\n",
    "* Run a brief hyper-parameter selction activity through a scalable grid search\n",
    "* Train and evaluate the predictive performance of recommendation system\n",
    "* Generate predictions from the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have seen how recommender/Recommendation Systems have played an  integral parts in the success of Amazon (Books, Items), Pandora/Spotify (Music), Google (News, Search), YouTube (Videos) etc.  For Amazon these systems bring more than 30% of their total revenues. For Netflix service, 75% of movies that people watch are based on some sort of recommendation.\n",
    "\n",
    "> The goal of Recommendation Systems is to find what is likely to be of interest to the user. This enables organizations to offer a high level of personalization and customer tailored services.\n",
    "\n",
    "\n",
    "For online video content services like Netflix and Hulu, the need to build robust movie recommendation systems is extremely important. An example of recommendation system is such as this:\n",
    "\n",
    "1.    User A watches Game of Thrones and Breaking Bad.\n",
    "2.    User B performs a search query for Game of Thrones.\n",
    "3.    The system suggests Breaking Bad to user B from data collected about user A.\n",
    "    \n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*Zvwzw_KPRv5bcXPkb6WubA.jpeg\" width=300>\n",
    "\n",
    "\n",
    "This lab will guide you through a step-by-step process into developing such a movie recommendation system. We shall use the MovieLens dataset to build a movie recommendation system using collaborative filtering technique with Spark's Alternating Least Saqures implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Data Acquisition and Pre-processing\n",
    "\n",
    "### MovieLens Ratings Dataset\n",
    "\n",
    "Previously we used the 1 million ratings dataset for applying simple SVD in python. \n",
    "\n",
    "For our experiment , we shall use the latest datasets. Grouplens offer the complete ratings dataset and a smaller subset for hyper parameter optmization and model selection. \n",
    "\n",
    "* **Complete Dataset**: 26,000,000 ratings applied to 45,000 movies by 270,000 users. Last updated 8/2017.\n",
    "* **Small Dataset**: 100,000 ratings applied to 9,000 movies by 700 users. Last updated 10/2016.\n",
    "\n",
    "Both these datasets are available in this repo. \n",
    "\n",
    "For this lab, we will use the small dataset `datasets/ms-latest-small/`. The main reason for using this dataset is to speed up computation and focus more on the pyspark programming. You are required to re run the experiment using the larger dataset in `dataset/ml-latest`.\n",
    "\n",
    "* Create a folder `datasets` in your pyspark environment. \n",
    "* Paste the provided ofiles in their respective directories as shown below\n",
    "* We will also keep the complete dataset for later experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import PySpark to our Python environment and and initiate a local SparkContext `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]') # [*] represents a local context i.e. no cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Selection, Parsing and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our SparkContext initialized, and our dataset in an accessible locations, we can now parse the csv files and read them into RDDs. The small dataset contains a number of csv file with features as shown below:  \n",
    "\n",
    "- ratings.csv :**UserID, MovieID, Rating, Timestamp**\n",
    "\n",
    "- movies.csv :**MovieID, Title, Genres > *Genre1|Genre2|Genre3...**\n",
    "\n",
    "- tags.csv :**UserID, MovieID, Tag, Timestamp**\n",
    "\n",
    "- links.csv :**MovieID, ImdbID, TmdbID**\n",
    "\n",
    "We shall focus on `ratings.csv`, and `movies.csv` from small dataset here for building our recommendation system. Other features can be incorporated later for improving the predictive performance of the system.  The format of these files is uniform and simple and such comma delimited files can be easily parsed line by line using Python `split()` once they are loaded as RDDs. \n",
    "\n",
    "Let's parse `ratings.csv` and `movies.csv` files into two RDDs. We also need to filter out the header row in each file containing column names. \n",
    "\n",
    "- For each line in the `ratings.csv`, create a tuple of (UserID, MovieID, Rating). Drop the Timestamp feature. \n",
    "- For each line in the `movies.csv`, Create a tuple of (MovieID, Title). Drop the Genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path variables for `ratings` and `movies` csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path for identifying the ratings and movies files in small dataset\n",
    "ratingsPath = 'datasets/ml-latest-small/ratings.csv'\n",
    "moviesPath = 'datasets/ml-latest-small/movies.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing `ratings.csv`\n",
    "\n",
    "- Use `sc.textFile()` to read the raw contents of ratings file into an RDD\n",
    "- Read the contents of ratings file into an RDD and view its first row (Header containing column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'userId,movieId,rating,timestamp'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsRaw = sc.textFile(ratingsPath)\n",
    "\n",
    "ratingsHeader = ratingsRaw.take(1)[0]\n",
    "ratingsHeader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to filter some of the data at this stage. We can drop the timestamp feature as we won't use that build recommendations. We will parse the raw ratings data into a new RDD and filter out the header row. Perform following transformations on `ratingsRaw`:\n",
    "\n",
    "1. Read `ratingsRaw` contents into a new RDD while using `.filter()` to exclude the header information with `ratinsHeader`.\n",
    "2. Split each line of the csv file using `,` demiliter as the input argument with `split()` function.\n",
    "3. Collect the first three elements of each row (UserID, MovieID, Rating) and discard fourth (timestep) field. Remember to read ratings as `float`. \n",
    "4. Cache the final RDD (Optional) using `RDD.cache()` (optional but help speed up computation with large RDDs).\n",
    "5. Print the total number of recommendations and view first three rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 100004 recommendations in the dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 31, 2.5), (1, 1029, 3.0), (1, 1061, 3.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsNoHeader = ratingsRaw.filter(lambda x: x != ratingsHeader)\n",
    "ratingsSplit = ratingsNoHeader.map(lambda x: x.split(','))\n",
    "ratingsRDD = ratingsSplit.map(lambda x: (int(x[0]), int(x[1]), float(x[2])))\n",
    "ratingsRDD.cache()\n",
    "print(\"there are %s recommendations in the dataset\" %(ratingsRDD.count()))\n",
    "ratingsRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks well in-line with our expectations. Let's do the same for `movies.csv`.\n",
    "\n",
    "### Parsing `movies.csv`\n",
    "\n",
    "We shall now proceed in a similar way with `movies.csv` file. Repeat following steps as performed above:\n",
    "\n",
    "1. Use the path variable for identifying the location of **movies.csv**.\n",
    "2. Read the text file into RDD.\n",
    "3. Exclude the header information.\n",
    "4. Split the line contents of the csv file.\n",
    "5. Read the contents of resulting RDD creating a (MovieID, Title) tuple and discard genres. \n",
    "6. Count number of movies in the final RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9125 movies in the complete dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 'Toy Story (1995)'),\n",
       " ('2', 'Jumanji (1995)'),\n",
       " ('3', 'Grumpier Old Men (1995)')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesRaw = sc.textFile(moviesPath)\n",
    "moviesHeader = moviesRaw.take(1)[0]\n",
    "\n",
    "# A good way to keep the code readable is to use one liners as below. You can choose to create new RDDs at each step.\n",
    "moviesRDD = moviesRaw.filter(lambda line: line != moviesHeader)\\\n",
    "                          .map(lambda line: line.split(\",\"))\\\n",
    "                          .map(lambda x: (x[0], x[1]))\\\n",
    "                          .cache()\n",
    "\n",
    "print ('There are' , moviesRDD.count(), 'movies in the complete dataset')\n",
    "\n",
    "moviesRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the two RDDs we created above and we shall use these to build and train our recommendation system. \n",
    "\n",
    "### Saving Pre-Processed Data (optional)\n",
    "\n",
    "We can optionally save our preprocessed datasets now or a later stage to avoid reading and re-processing large files repeatedly. Create a folder \"pre_processed\" and save `movieRDD` and `ratingsRDD` using `RDD.saveAsTExtFile(filename)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "processedPath = 'pre_processed'\n",
    "os.mkdir(processedPath)\n",
    "\n",
    "moviesRDD.saveAsTextFile(os.path.join(processedPath, 'moviesRDD'))\n",
    "ratingsRDD.saveAsTextFile(os.path.join(processedPath, 'ratingsRDD'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files can be read back using `sc.textfile()` method that we saw earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Alternate Least Squares: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data as Testing , Training and Validation Sets. \n",
    "\n",
    "We will now split the data for building our recommendation system. We can use spark's `randomsplit()` transformation that uses given weights to split an rdd into any number of sub-RDDs. The standared usage of this transformation function is :\n",
    "\n",
    "> `RDD.randomSplit(weights, seed)`\n",
    "\n",
    "**weights** – weights for splits, will be normalized if they don’t sum to 1\n",
    "\n",
    "**seed** – random seed\n",
    "\n",
    "Perform following tasks:\n",
    "\n",
    "- Split the `ratingsRDD` into training, testing and validation RDDs (60%, 20%, 20%) using respective weights.\n",
    "- Perform a `.count` on resulting datasets to show the count of elements of each RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60050, 19904, 20050)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRDD, validRDD, testRDD = ratingsRDD.randomSplit([6, 2, 2], seed=200)\n",
    "trainRDD.count(), testRDD.count(), validRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction of ratings, we would need `customerID` and `movieID` from validation and test RDDs respectively. \n",
    "\n",
    "- Map `customerID` and `movieID` from validation and test RDDs into two new RDDs which will be used for training and validation purpose. \n",
    "- Take 3 elements from both RDDs to inspect the results.\n",
    "\n",
    "For now , we shall ignore the `ratings` values for these RDDs, as these will be predicted later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Validation Features: [(1, 1263), (1, 1343), (1, 1405)]\n",
      "Example Test Features: [(1, 1129), (1, 2294), (1, 2968)]\n"
     ]
    }
   ],
   "source": [
    "validFeaturesRDD = validRDD.map(lambda x: (x[0], x[1]))\n",
    "testFeaturesRDD = testRDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "print ('Example Validation Features:', validFeaturesRDD.take(3))\n",
    "print ('Example Test Features:', testFeaturesRDD.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `validFeaturesRDD` during the training process to avoid the model from overfitting (getting stuck into a local minima) and the `testFeaturesRDD` with trained model to measure its predictive performance. Good old supervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering and Alternate Least Squares in Spark\n",
    "\n",
    "Spark MLlib library for Machine Learning provides a Collaborative Filtering implementation by using Alternating Least Squares (ALS) algorithm.\n",
    "\n",
    "`spark.mllib` currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. `spark.mllib` uses the alternating least squares (ALS) algorithm to learn these latent factors. \n",
    "\n",
    "We shall work with following hyper-parameters and set their values prior to the actual training process:\n",
    "\n",
    "* `rank` : Number of hidden/latent factors in the model. **(use the list [2,4,6,8,10] as possible rank values)**\n",
    "* `iterations` : Number of iterations to run. **(set to 10)**\n",
    "* `lambda` :  Regularization parameter in ALS.**(set to 0.1)**\n",
    "\n",
    "Spark offers a lot of other parameters for detailed and indepth fine tuning of the algorithm. Details on spark's ALS implementation can be viewed [HERE](https://spark.apache.org/docs/2.2.0/mllib-collaborative-filtering.html). For now, we will use default values for all the other hyper parameters. \n",
    "\n",
    "- Import ALS from `mllib` along with `math` module (for calculating `RMSE`) and set our learning parameters. \n",
    "- Import the ALS algorithm and set parameters as shown above. \n",
    "- For the initial experiment, use `iterations = 10`, `lambda = 0.1` and run a grid for identifying best value for `rank`.\n",
    "\n",
    "> **Note**: You may decide to run a larger grid with other model parameters after setting up the codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "# set learning parameters \n",
    "seed = 500\n",
    "numIterations = 10\n",
    "lambdaRegParam = 0.1\n",
    "ranksVal = [2, 4, 6, 8, 10]\n",
    "errVal = [0, 0, 0, 0, 0] # initialize a matrix for storing error values for selected ranks\n",
    "errIter = 0 # iterator for above list \n",
    "\n",
    "# Set training parameters\n",
    "minError = float('inf')\n",
    "bestRank = -1 \n",
    "bestIteration = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation with hyper-parameter optimization\n",
    "\n",
    "We can now start our training process using above parameter values which would include following steps: \n",
    "\n",
    "* Run the training for each of the rank values in our `ranks` list inside a for loop.\n",
    "* Train the model using trainRDD, ranksVal, seed, numIterations and lambdaRegParam value as model parameters. \n",
    "* Validate the trained model by predicting ratings for `validFeaturesRDD` using `ALS.predictAll()`.\n",
    "* Compare predicted ratings to actual ratings by joining generated predictions with `validRDD`. \n",
    "* Calculate error as RMSE for each rank. \n",
    "* Find the best rank value based on RMSE\n",
    "\n",
    "For sake of simplicity, we shall repeat training process for changing ranks value **only**. Other values can also be changed as a detailed grid search for improved predictive performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Rank = 2 , the RMSE value is: 0.9492876773915179\n",
      "For Rank = 4 , the RMSE value is: 0.94553209880163\n",
      "For Rank = 6 , the RMSE value is: 0.9491943433112304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "\n",
      "Exception happened during processing of request from ('127.0.0.1', 38534)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 816, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 714, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o11.setCallSite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ebe41683dd7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Calculate RMSE error for the difference between ratings and predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidPlusPreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# save error into errors array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \"\"\"\n\u001b[0;32m-> 1202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/traceback_utils.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o11.setCallSite"
     ]
    }
   ],
   "source": [
    "for r in ranksVal:\n",
    "    \n",
    "    # Train the model using trainRDD, rank, seed, iterations and lambda value as model parameters\n",
    "    movieRecModel = ALS.train(trainRDD, \n",
    "                              rank = r, \n",
    "                              seed = seed, \n",
    "                              iterations = numIterations,\n",
    "                              lambda_ = lambdaRegParam)\n",
    "    \n",
    "    # Use the trained model to predict the ratings from validPredictionRDD using model.predictAll()\n",
    "    predictions = movieRecModel.predictAll(validFeaturesRDD).map(lambda p: ((p[0], p[1]), p[2]))\n",
    "\n",
    "    # Compare predicted ratings and actual ratings in validRDD\n",
    "    validPlusPreds = validRDD.map(lambda p: ((int(p[0]), int(p[1])), float(p[2]))).join(predictions)\n",
    "    \n",
    "    # Calculate RMSE error for the difference between ratings and predictions\n",
    "    error = math.sqrt(validPlusPreds.map(lambda p: (p[1][0] - p[1][1])**2).mean())\n",
    "    \n",
    "    # save error into errors array\n",
    "    errVal[errIter] = error\n",
    "    errIter += 1\n",
    "    \n",
    "    print ('For Rank = %s , the RMSE value is: %s' % (r, error))\n",
    "    \n",
    "    # Check for best error and rank values\n",
    "    if error < minError:\n",
    "        minError = error\n",
    "        bestRank = r\n",
    "\n",
    "print ('The best model was trained with Rank = %s' % bestRank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Predictions\n",
    "\n",
    "- Have a look at the format of predictions the model generated during last validation stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [((580, 1084), 3.492776257690668),\n",
    "*  ((302, 1084), 3.078629750519478),\n",
    "*  ((514, 1084), 3.985426769882686)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows we have the `((UserID,  MovieID), Rating)` tuple, similar to the ratings dataset. The `Ratings` field in the predictions RDD refers to the ratings predicted by the trained ALS model. \n",
    "\n",
    "Above code joins these predictions with our validation data\n",
    "- take 3 elements from the validPlusPreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1405), (1.0, 2.7839431097640492)),\n",
       " ((2, 296), (4.0, 3.9729953606585244)),\n",
       " ((2, 616), (3.0, 3.128218990007167))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validPlusPreds.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows the format `((UserId, MovieId), Ratings, PredictedRatings)`. \n",
    "\n",
    "We calculated the RMSE by taking the squred difference and calculating the mean value as our `error` value.\n",
    "\n",
    "### Testing the Model\n",
    "\n",
    "We shall now test the model with test dataset that has been kept away from the learning phase upto this point. \n",
    "Use following parameters:\n",
    "\n",
    "* Use `trainRDD` for training the model.\n",
    "* Use `bestRank` value learnt during the validation phase.\n",
    "* Use other parameter values same as above. \n",
    "* Generate predictions with `testFeaturesRDD`\n",
    "* Calculate error between predicted values and ground truth as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing dataset, the calculated RMSE value:  0.9498348141480232\n"
     ]
    }
   ],
   "source": [
    "movieRecModel = ALS.train(trainRDD, \n",
    "                           bestRank, \n",
    "                           seed = seed, \n",
    "                           iterations = numIterations,\n",
    "                           lambda_ = lambdaRegParam)\n",
    "\n",
    "# Calculate predictions for testPredictionRDD\n",
    "predictions = movieRecModel.predictAll(testFeaturesRDD).map(lambda x: ((x[0], x[1]), x[2]))\n",
    "\n",
    "# Combine real ratings and predictions\n",
    "testPlusPreds = testRDD.map(lambda x: ((int(x[0]), int(x[1])), float(x[2]))).join(predictions)\n",
    "\n",
    "# Calculate RMSE\n",
    "error = math.sqrt(testPlusPreds.map(lambda x: (x[1][0] - x[1][1])**2).mean())\n",
    "print ('For testing dataset, the calculated RMSE value:  %s' % (error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesnt look so bad. Remember due to probablistic nature of ALS algorithm, changing the seed value will also show somen fluctuations in RMSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Making Recommendations\n",
    "\n",
    "For making recommendations using collaborative filtering, we re-train the model including the new user preferences in order to compare them with other users in the dataset. In simple terms, the system needs to be trained every time we have new user ratings. Once we have our model trained, we can reuse it to obtain top recomendations for a given user or an individual rating for a particular movie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to count the number of ratings per movie. We can create a function that inputs the movies RDD created earlier  and calculates total number of ratings. Based on this, we can later define a threshold ratings value to only include movies with a minimum count of ratings. \n",
    "\n",
    "Create a function `getRatingCount()` to do following:\n",
    "\n",
    "* Input the ratings RDD (grouped by movies)\n",
    "* Count the total number of rating for a given movie ID\n",
    "* Return the movie id and total number of ratings as a tuple. \n",
    "\n",
    "\n",
    "Perform following tasks in the given sequence: \n",
    "\n",
    "* Use `ratingsRDD` to get movie ID and ratings values, and `groupby()` movie ID to group all ratings for each movie\n",
    "* Pass the new RDD to the function above to count the number of ratings for all movies\n",
    "* create a new RDD `movieRatingsCountsRDD` to carry movie rating and count as a tuple\n",
    "* take 5 elements for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1172, 46), (2150, 36), (2294, 53), (2968, 43), (10, 122)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getRatingCount(IDRatings):\n",
    "    nRatings = len(IDRatings[1])\n",
    "    return IDRatings[0], nRatings\n",
    "\n",
    "movieRatingsCountsRDD = ratingsRDD.map(lambda x: (x[1], x[2])).groupByKey().map(getRatingCount)\n",
    "movieRatingsCountsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding New User(s) and Rating(s)\n",
    "\n",
    "In order to make recommendations, we now need to create a new user and generate some initial set of ratings for collaborative filtering to work. First let's create a new user with a unique id , say 0, as its not currently used and would be easily identifiable later. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "newUserID = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to rate some movies under this user. You are encouraged to look into movies RDD to set some ratings for the movies based on your own preferences. That would give you a good chance to qualitatively assess the the outcome for this system. \n",
    "\n",
    "For this experiment, lets create some rating values for our new user who likes comedy, family and romantic movies. You can add or omit other ratings too. \n",
    "\n",
    "    18\t    Four Rooms (1995)\tComedy\n",
    "    60074\tHancock (2008)\tAction|Adventure|Comedy|Crime|Fantasy\n",
    "    19\t    Ace Ventura: When Nature Calls (1995)\tComedy\n",
    "    203\t    To Wong Foo, Thanks for Everything! Julie Newmar (1995)\tComedy\n",
    "    205\t    Unstrung Heroes (1995)\tComedy|Drama\n",
    "    8784\tGarden State (2004)\tComedy|Drama|Romance\n",
    "    55830\tBe Kind Rewind (2008)\tComedy\n",
    "    56176\tAlvin and the Chipmunks (2007)\tChildren|Comedy\n",
    "    63393\tCamp Rock (2008)\tComedy|Musical|Romance\n",
    "    64622\tReader, The (2008)\tDrama|Romance\n",
    "    65088\tBedtime Stories (2008)\tAdventure|Children|Comedy\n",
    "    78499\tToy Story 3 (2010)\tAdventure|Animation|Children|Comedy|Fantasy|IMAX\n",
    "\n",
    "We will put these ratings in a new RDD use the user ID = -1 to create a (userID, movieID, rating) tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on above, create an RDD containing (userID, movieID, rating) tuple\n",
    "newUserRating = [(0,18,4),\n",
    "                 (0,60074,5),\n",
    "                 (0,19,4),\n",
    "                 (0,203,3),\n",
    "                 (0,205,4),\n",
    "                 (0,8784,5),\n",
    "                 (0,55830,3),\n",
    "                 (0,63393,4),\n",
    "                 (0,64622,5) ,\n",
    "                 (0,78499,5)]\n",
    "\n",
    "newUserRDD = sc.parallelize(newUserRating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great. We can now combine the `newUserRDD` with `moviesRDD` using a `.union()` transformation to make it a part of MovieLense dataset. Its always a good idea to check the results with `.take()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 31, 2.5), (1, 1029, 3.0), (1, 1061, 3.0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataWithNewUser = ratingsRDD.union(newUserRDD)\n",
    "dataWithNewUser.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the ALS model again, using all the parameters we selected before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with `dataWithNewRating` and parameters used earlier.\n",
    "newRatingsModel  = ALS.train(dataWithNewUser, \n",
    "                              rank = bestRank, \n",
    "                              seed=seed, \n",
    "                              iterations=numIterations, \n",
    "                              lambda_= lambdaRegParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall need to repeat that every time a user adds new ratings. Ideally we will do this in batches, and not for every single rating that comes into the system for every user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Top Recomendations\n",
    "\n",
    "After traning the model with our new user and ratings, we can finally get some recommendations. For that we will make an RDD with all the movies the new user hasn't rated yet.\n",
    "\n",
    "For this stage, perform following transformations:\n",
    "* Create a `moviesTitles` RDD with tuples as (id, title) from `moviesRDD`. Confirm the output.\n",
    "* Make an RDD to just the IDs of the movies rated by the new user above in `newUserRating`. \n",
    "* Filter the `moviesRDD` into a new RDD `newUserUnratedRDD` to only contain those movies not rated by the user.\n",
    "* Use `newUserUnratedRDD` and predict the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Toy Story (1995)'),\n",
       " (2, 'Jumanji (1995)'),\n",
       " (3, 'Grumpier Old Men (1995)')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a movieTitles RDD with (movie,title) tuple\n",
    "moviesTitles = moviesRDD.map(lambda x: (int(x[0]),x[1]))\n",
    "moviesTitles.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=1084, rating=3.5312425479995),\n",
       " Rating(user=0, product=7942, rating=3.598790878082731),\n",
       " Rating(user=0, product=6400, rating=3.1097929195008023)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get just movie IDs for new user rated movies\n",
    "newUserRatingsIds = map(lambda x: x[1], newUserRating) \n",
    "\n",
    "# Filter out complete movies RDD \n",
    "newUserUnratedRDD = (moviesRDD.filter(lambda x: x[0] not in newUserRatingsIds).map(lambda x: (newUserID, x[0])))\n",
    "\n",
    "# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "newRecRDD = newRatingsModel.predictAll(newUserUnratedRDD)\n",
    "\n",
    "newRecRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new recommendation RDD `newRecRDD` now carries the predicted recommendations for new user. Now we can now look at top x number of movies with the highest predicted ratings and join these with the movies RDD to get the titles, and ratings count to make the results more meaningful. \n",
    "\n",
    "For this you need to perform following tasks:\n",
    "\n",
    "* Map `newRecRDD` to build a (movie, ratings) tuple for each entry as `newRecRatingRDD`\n",
    "* Use `.join()` transformation sequentially to to join `newRecRatingRDD` to `moviesTitles` and to `movieRatingsCountsRDD` to create \n",
    "\n",
    "A good resource on PySpark `.join()` is available at [THIS](http://www.learnbymarketing.com/1100/pyspark-joins-by-example/) resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
    "newRecRatingRDD = newRecRDD.map(lambda x: (x.product, x.rating))\n",
    "newRecRatingTitleCountRDD = newRecRatingRDD.join(moviesTitles)\\\n",
    "                                           .join(movieRatingsCountsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simplify the the above to only include **(title, ratings, count)** and transform as a new RDD containing new ratings for unrated movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rubber (2010)', 4.715666341687083, 1),\n",
       " ('Proof (1991)', 3.9517041049162795, 1),\n",
       " ('Under Siege 2: Dark Territory (1995)', 3.118223921273012, 31)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newRecRatingTitleCountFlatRDD = newRecRatingTitleCountRDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))\n",
    "newRecRatingTitleCountFlatRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINALLY, we can get highest rated recommended movies for the new user, filtering out movies with less than 50 ratings (try changing this value).\n",
    "For this we need to do following: \n",
    "\n",
    "* Use `.filter()` to include only the movies with more than 50 ratings.\n",
    "* Use `.takeordered()` to get top 10 recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP recommended movies (with more than 50 reviews):\n",
      "('\"Shawshank Redemption', 5.447804190989062, 311)\n",
      "('V for Vendetta (2006)', 5.432833918216835, 73)\n",
      "('Harry Potter and the Goblet of Fire (2005)', 5.424466636277112, 59)\n",
      "('Life Is Beautiful (La Vita è bella) (1997)', 5.384201632659801, 99)\n",
      "('\"Lock', 5.380165378272083, 74)\n",
      "('Forrest Gump (1994)', 5.337304995573618, 341)\n",
      "('\"Princess Bride', 5.328423741235671, 163)\n",
      "('Good Will Hunting (1997)', 5.301483354034365, 157)\n",
      "('\"Breakfast Club', 5.234274895183267, 117)\n",
      "('Slumdog Millionaire (2008)', 5.227081955573315, 52)\n"
     ]
    }
   ],
   "source": [
    "recommendations = newRecRatingTitleCountFlatRDD.filter(lambda x: x[2]>=50)\n",
    "top10MoviesOrdered = recommendations.takeOrdered(10, key=lambda y: -y[1])\n",
    "\n",
    "print ('TOP recommended movies (with more than 50 reviews):\\n%s' %\n",
    "        '\\n'.join(map(str, top10MoviesOrdered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can also check bottom 10 movies with lowest ratings with `.takeOrdered()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest recommended movies (with more than 50 reviews):\n",
      "('Beverly Hills Cop III (1994)', 2.423247696283056, 57)\n",
      "('\"Blair Witch Project', 2.456475591917372, 86)\n",
      "('Bowfinger (1999)', 2.495144318199298, 51)\n",
      "('\"Cable Guy', 2.633730093117032, 59)\n",
      "('Congo (1995)', 2.784807232020519, 63)\n",
      "('Species (1995)', 2.831861058132409, 55)\n",
      "('Judge Dredd (1995)', 2.8391230652113846, 70)\n",
      "('Mighty Aphrodite (1995)', 2.845570668091761, 51)\n",
      "('Casper (1995)', 2.855333652701143, 58)\n",
      "('Executive Decision (1996)', 3.0047635050446324, 61)\n"
     ]
    }
   ],
   "source": [
    "bottom10MoviesOrdered = recommendations.takeOrdered(10, key=lambda y: y[1])\n",
    "print ('Lowest recommended movies (with more than 50 reviews):\\n%s' %\n",
    "        '\\n'.join(map(str, bottom10MoviesOrdered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have it. Our recommendation system is generating quite meaningful results with top 10 movies. A qualitative and subjective assessment shows top 10 movies are generally with a comedy/family/romance themes while the bottom 10 movies include some sci-fi/horror/action movies. \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "* Run the hyperparameter selection section again trying different values for the lambda regulizer. Use a nested for loop to try different values for Rank and Lambda. Inspect any improvement in accuracy. \n",
    "\n",
    "\n",
    "* Include other features in your recommender e.g. Genres and tags (Needs dummy variables), IMDB ratings etc.\n",
    "\n",
    "\n",
    "* **Remember these results are only based on a subset of data. Run the code with complete dataset in a similar fashion and discuss the improvement and predictive performance through RMSE, as well as subjective evaluations based on your personal preferences / taste in movies.**\n",
    "\n",
    "\n",
    "## Level up - Optional \n",
    "\n",
    "* Use IMDB links to scrap user reviews from IMDB and using basic NLP techniques (word2vec), create extra embeddings for ALS model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we learnt how to build a model using Spark, how to perform some parameter selection using a reduced dataset, and how to update the model every time that new user preferences come in. We looked at how Spark's ALS implementation can be be used to build a scalable and efficient reommendation system. We also saw that such systems can become computationaly expensive and using them with an online system could be a problem with traditional computational platforms. Spark's disctributed computing architecture provides a great solution to deploy such recommendation systems for real worls applications (think Amazon, Spotify)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
